{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.31445   0.662022]\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "import math\n",
    "from math import exp\n",
    "\n",
    "def actF(x):\n",
    "    act = math.tanh(x)\n",
    "    return [act,1-act*act]\n",
    "\n",
    "def cross_entropy_loss(output, target):\n",
    "    return (output - target)\n",
    "\n",
    "def preprocess(filename):\n",
    "    file = open(filename,'r')\n",
    "    dataset = [line.split() for line in file]\n",
    "    train_data = [np.array([float(row[0]),float(row[1])]) for row in dataset]\n",
    "    \n",
    "    train_labels = np.zeros((len(train_data),2))\n",
    "    labels = [int(row[2]) for row in dataset]\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "            train_labels[i][labels[i]] = 1    \n",
    "    \n",
    "    return train_data, train_labels\n",
    "\n",
    "def softmax_norm(output_vec):\n",
    "    l = [exp(x) for x in output_vec]\n",
    "    npl = np.array(l)\n",
    "    mysum = npl.sum()\n",
    "    Dr = np.array([mysum for i in range(0,len(output_vec))])\n",
    "    return (npl/Dr)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        init_weights = [[random() for x in range(n_input+1)] for i in range(n_hidden)]\n",
    "        init_weights = np.array(init_weights)\n",
    "        self.hidden_layer = {'W':init_weights}\n",
    "        init_weights = [[random() for x in range(n_hidden+1)] for i in range(n_output)]\n",
    "        init_weights = np.array(init_weights)\n",
    "        self.output_layer = {'W' : init_weights}\n",
    "        self.layers = 2\n",
    "        \n",
    "    def forward_prop(self, input_vector):\n",
    "        inputs = np.append(input_vector,1)\n",
    "        w_mat = nn.hidden_layer['W']\n",
    "        prod = np.matmul(w_mat.reshape(3,4).transpose(),inputs.reshape(3,1))\n",
    "        act = []\n",
    "        delta = []\n",
    "        for i in prod:\n",
    "            act.append(actF(i)[0])\n",
    "            delta.append(actF(i)[1])\n",
    "        nn.hidden_layer['delta'] = np.array(delta)\n",
    "        new_inputs = act\n",
    "        new_inputs = np.append(new_inputs,1)\n",
    "        w_mat = nn.output_layer['W']\n",
    "        prod = np.matmul(w_mat.reshape(5,2).transpose(),new_inputs.reshape(5,1))\n",
    "        act = []\n",
    "        delta = []\n",
    "        for i in prod:\n",
    "            act.append(actF(i)[0])\n",
    "            delta.append(actF(i)[1])\n",
    "        nn.output_layer['delta'] = np.array(delta)\n",
    "        return act\n",
    "    \n",
    "    def backward_prop(self, loss):\n",
    "        loss = loss.reshape(2,1)\n",
    "        error_grad = [a*b for a,b in zip(loss,nn.output_layer['delta'])]\n",
    "        learning_rate = 0.1\n",
    "        error_grad = np.array(error_grad).reshape(2,1)\n",
    "        nn.output_layer['W'] = nn.output_layer['W'] - learning_rate*error_grad\n",
    "        \n",
    "        error_grad1 = nn.hidden_layer['delta'].reshape(4,1) * error_grad.sum().transpose()\n",
    "        nn.hidden_layer['W'] = nn.hidden_layer['W'] - learning_rate* error_grad1\n",
    "      \n",
    "    \n",
    "    def forward_prop1(self, input_vector):\n",
    "        print(input_vector)\n",
    "    \n",
    "    def train_network(self, train_data, train_labels):\n",
    "        train_data, train_labels = preprocess('dataset.txt')\n",
    "        row_count = 0;\n",
    "        for row in train_data:\n",
    "            if row_count == 1 : break\n",
    "            error = []\n",
    "            output = self.forward_prop1(row)\n",
    "            output = softmax_norm(output)\n",
    "            loss = cross_entropy_loss(train_labels[row_count], output)\n",
    "            nn.backward_prop(loss)\n",
    "            print(\"loss :\", loss)\n",
    "#             output = softmax_norm(output)\n",
    "        \n",
    "nn = NeuralNetwork(2,5,2)\n",
    "train_data, train_labels = preprocess('dataset.txt')\n",
    "nn.train_network(train_data, train_labels)\n",
    "\n",
    "    \n",
    "    \n",
    "# x = np.array([1,2,3,4])\n",
    "# # print(x.sum())\n",
    "# dataset = [[1,2],[2,1]]\n",
    "# labels = [1,0]\n",
    "# np.append(x,5)\n",
    "\n",
    "# train_data, train_labels = preprocess('dataset.txt')\n",
    "\n",
    "# nn = NeuralNetwork(2,4,2)\n",
    "# # print(nn.hidden_layer['W'])\n",
    "# # print(nn.output_layer['W'])\n",
    "# # nn.forward_prop()\n",
    "# row_count = 0;\n",
    "# sum_error = 0;\n",
    "# for row in train_data:\n",
    "#     if row_count == 1: break\n",
    "#     error = []\n",
    "#     output = nn.forward_prop(row)\n",
    "#     output = softmax_norm(output)\n",
    "#     loss = cross_entropy_loss(train_labels[row_count], output)\n",
    "#     nn.backward_prop(loss)\n",
    "#     sum_error += sum([(train_labels[row_count] - output)**2 for i in range(len(output))])\n",
    "#     print(\"loss :\", loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.09898757]\n",
      " [0.08782891]]\n",
      "[[-0.09898757 -0.09898757 -0.09898757 -0.09898757 -0.09898757]\n",
      " [-0.08782891 -0.08782891 -0.08782891 -0.08782891 -0.08782891]]\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros([2,5])\n",
    "x = np.random.rand(2,1)\n",
    "n = 0.1\n",
    "print(w)\n",
    "print(0.1*x)\n",
    "print(w-0.1*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
